{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc7e92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_cv\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import requests\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Conv2D, BatchNormalization, Activation, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71ec6b66",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 191. MiB for an array with shape (25000000,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflatland_train.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m y[y \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m    \u001b[38;5;66;03m# Correct labels so that triangle is mapped to class 1\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\npyio.py:245\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\format.py:768\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfromfile(fp, dtype\u001b[38;5;241m=\u001b[39mdtype, count\u001b[38;5;241m=\u001b[39mcount)\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m--> 768\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    771\u001b[0m         \u001b[38;5;66;03m# If dtype.itemsize == 0 then there's nothing more to read\u001b[39;00m\n\u001b[0;32m    772\u001b[0m         max_read_count \u001b[38;5;241m=\u001b[39m BUFFER_SIZE \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmin\u001b[39m(BUFFER_SIZE, dtype\u001b[38;5;241m.\u001b[39mitemsize)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 191. MiB for an array with shape (25000000,) and data type float64"
     ]
    }
   ],
   "source": [
    "data = np.load('flatland_train.npz')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "y[y != 0] -= 2    # Correct labels so that triangle is mapped to class 1\n",
    "X = X / 255.      # Scale down to range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b84a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_turbo=np.concatenate((X,X,X,X,X,X))\n",
    "y_turbo=np.concatenate((y,y,y,y,y,y))\n",
    "X_turbo = np.expand_dims(X_turbo, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fdc735",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.55,\n",
    "    height_shift_range=0.55,\n",
    "    zoom_range = [0.35, 1.65],\n",
    "    shear_range = 13,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8915b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 4e-4\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.98,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f75cec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_84 (Conv2D)          (None, 42, 42, 16)        1312      \n",
      "                                                                 \n",
      " conv2d_85 (Conv2D)          (None, 40, 40, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d_39 (MaxPoolin  (None, 20, 20, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_86 (Conv2D)          (None, 18, 18, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_87 (Conv2D)          (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_40 (MaxPoolin  (None, 8, 8, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_88 (Conv2D)          (None, 6, 6, 64)          18496     \n",
      "                                                                 \n",
      " conv2d_89 (Conv2D)          (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_41 (MaxPoolin  (None, 2, 2, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 75,514\n",
      "Trainable params: 75,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "1875/1875 [==============================] - 24s 12ms/step - loss: 1.3280 - accuracy: 0.4261\n",
      "Epoch 2/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.9598 - accuracy: 0.6148\n",
      "Epoch 3/150\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.8440 - accuracy: 0.6712\n",
      "Epoch 4/150\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.7898 - accuracy: 0.6953\n",
      "Epoch 5/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.7487 - accuracy: 0.7131\n",
      "Epoch 6/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.7275 - accuracy: 0.7214\n",
      "Epoch 7/150\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.7011 - accuracy: 0.7319\n",
      "Epoch 8/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6924 - accuracy: 0.7346\n",
      "Epoch 9/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6754 - accuracy: 0.7430\n",
      "Epoch 10/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6662 - accuracy: 0.7461\n",
      "Epoch 11/150\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6444 - accuracy: 0.7535\n",
      "Epoch 12/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6378 - accuracy: 0.7572\n",
      "Epoch 13/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6329 - accuracy: 0.7569\n",
      "Epoch 14/150\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6254 - accuracy: 0.7614\n",
      "Epoch 15/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6196 - accuracy: 0.7626\n",
      "Epoch 16/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6150 - accuracy: 0.7647\n",
      "Epoch 17/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.6045 - accuracy: 0.7687\n",
      "Epoch 18/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5977 - accuracy: 0.7692\n",
      "Epoch 19/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5914 - accuracy: 0.7731\n",
      "Epoch 20/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5889 - accuracy: 0.7748\n",
      "Epoch 21/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5844 - accuracy: 0.7767\n",
      "Epoch 22/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5808 - accuracy: 0.7781\n",
      "Epoch 23/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5796 - accuracy: 0.7792\n",
      "Epoch 24/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5736 - accuracy: 0.7789\n",
      "Epoch 25/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5655 - accuracy: 0.7836\n",
      "Epoch 26/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5646 - accuracy: 0.7837\n",
      "Epoch 27/150\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.5632 - accuracy: 0.7847\n",
      "Epoch 28/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5640 - accuracy: 0.7839\n",
      "Epoch 29/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5590 - accuracy: 0.7861\n",
      "Epoch 30/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5501 - accuracy: 0.7885\n",
      "Epoch 31/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5557 - accuracy: 0.7869\n",
      "Epoch 32/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5582 - accuracy: 0.7871\n",
      "Epoch 33/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5409 - accuracy: 0.7929\n",
      "Epoch 34/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5439 - accuracy: 0.7908\n",
      "Epoch 35/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5496 - accuracy: 0.7886\n",
      "Epoch 36/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5450 - accuracy: 0.7915\n",
      "Epoch 37/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5410 - accuracy: 0.7941\n",
      "Epoch 38/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5338 - accuracy: 0.7944\n",
      "Epoch 39/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5380 - accuracy: 0.7934\n",
      "Epoch 40/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5360 - accuracy: 0.7962\n",
      "Epoch 41/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5349 - accuracy: 0.7955\n",
      "Epoch 42/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5245 - accuracy: 0.7997\n",
      "Epoch 43/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5333 - accuracy: 0.7953\n",
      "Epoch 44/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5334 - accuracy: 0.7951\n",
      "Epoch 45/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5212 - accuracy: 0.8012\n",
      "Epoch 46/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5234 - accuracy: 0.8009\n",
      "Epoch 47/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5229 - accuracy: 0.8018\n",
      "Epoch 48/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5197 - accuracy: 0.8021\n",
      "Epoch 49/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5273 - accuracy: 0.7981\n",
      "Epoch 50/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5212 - accuracy: 0.8000\n",
      "Epoch 51/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5191 - accuracy: 0.7996\n",
      "Epoch 52/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5191 - accuracy: 0.8016\n",
      "Epoch 53/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5151 - accuracy: 0.8054\n",
      "Epoch 54/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5176 - accuracy: 0.8020\n",
      "Epoch 55/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5090 - accuracy: 0.8048\n",
      "Epoch 56/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5158 - accuracy: 0.8047\n",
      "Epoch 57/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5109 - accuracy: 0.8039\n",
      "Epoch 58/150\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.5124 - accuracy: 0.8038\n",
      "Epoch 59/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5093 - accuracy: 0.8076\n",
      "Epoch 60/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5068 - accuracy: 0.8075\n",
      "Epoch 61/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5081 - accuracy: 0.8073\n",
      "Epoch 62/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5052 - accuracy: 0.8066\n",
      "Epoch 63/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5069 - accuracy: 0.8069\n",
      "Epoch 64/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5106 - accuracy: 0.8077\n",
      "Epoch 65/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5044 - accuracy: 0.8083\n",
      "Epoch 66/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5036 - accuracy: 0.8095\n",
      "Epoch 67/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5053 - accuracy: 0.8065\n",
      "Epoch 68/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5012 - accuracy: 0.8113\n",
      "Epoch 69/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5006 - accuracy: 0.8105\n",
      "Epoch 70/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5005 - accuracy: 0.8089\n",
      "Epoch 71/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4977 - accuracy: 0.8115\n",
      "Epoch 72/150\n",
      "1875/1875 [==============================] - 23s 13ms/step - loss: 0.4979 - accuracy: 0.8104\n",
      "Epoch 73/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4996 - accuracy: 0.8091\n",
      "Epoch 74/150\n",
      "1875/1875 [==============================] - 23s 13ms/step - loss: 0.4991 - accuracy: 0.8108\n",
      "Epoch 75/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4921 - accuracy: 0.8123\n",
      "Epoch 76/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4943 - accuracy: 0.8128\n",
      "Epoch 77/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4919 - accuracy: 0.8128\n",
      "Epoch 78/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4904 - accuracy: 0.8137\n",
      "Epoch 79/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4948 - accuracy: 0.8117\n",
      "Epoch 80/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4946 - accuracy: 0.8106\n",
      "Epoch 81/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4902 - accuracy: 0.8136\n",
      "Epoch 82/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4890 - accuracy: 0.8145\n",
      "Epoch 83/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4866 - accuracy: 0.8137\n",
      "Epoch 84/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4888 - accuracy: 0.8156\n",
      "Epoch 85/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4895 - accuracy: 0.8159\n",
      "Epoch 86/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4909 - accuracy: 0.8132\n",
      "Epoch 87/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4910 - accuracy: 0.8128\n",
      "Epoch 88/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4969 - accuracy: 0.8117\n",
      "Epoch 89/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4851 - accuracy: 0.8165\n",
      "Epoch 90/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4894 - accuracy: 0.8127\n",
      "Epoch 91/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4876 - accuracy: 0.8146\n",
      "Epoch 92/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4840 - accuracy: 0.8161\n",
      "Epoch 93/150\n",
      "1875/1875 [==============================] - 24s 12ms/step - loss: 0.4814 - accuracy: 0.8172\n",
      "Epoch 94/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4824 - accuracy: 0.8152\n",
      "Epoch 95/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4786 - accuracy: 0.8195\n",
      "Epoch 96/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4879 - accuracy: 0.8135\n",
      "Epoch 97/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4801 - accuracy: 0.8194\n",
      "Epoch 98/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4859 - accuracy: 0.8124\n",
      "Epoch 99/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4853 - accuracy: 0.8151\n",
      "Epoch 100/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4840 - accuracy: 0.8161\n",
      "Epoch 101/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4824 - accuracy: 0.8176\n",
      "Epoch 102/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4798 - accuracy: 0.8180\n",
      "Epoch 103/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4796 - accuracy: 0.8181\n",
      "Epoch 104/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4785 - accuracy: 0.8201\n",
      "Epoch 105/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4745 - accuracy: 0.8198\n",
      "Epoch 106/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4771 - accuracy: 0.8195\n",
      "Epoch 107/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4795 - accuracy: 0.8165\n",
      "Epoch 108/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4792 - accuracy: 0.8181\n",
      "Epoch 109/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4779 - accuracy: 0.8190\n",
      "Epoch 110/150\n",
      "1875/1875 [==============================] - 23s 13ms/step - loss: 0.4721 - accuracy: 0.8220\n",
      "Epoch 111/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4762 - accuracy: 0.8199\n",
      "Epoch 112/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4737 - accuracy: 0.8194\n",
      "Epoch 113/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4733 - accuracy: 0.8207\n",
      "Epoch 114/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4751 - accuracy: 0.8202\n",
      "Epoch 115/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4731 - accuracy: 0.8201\n",
      "Epoch 116/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4756 - accuracy: 0.8205\n",
      "Epoch 117/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4686 - accuracy: 0.8219\n",
      "Epoch 118/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4778 - accuracy: 0.8192\n",
      "Epoch 119/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4681 - accuracy: 0.8209\n",
      "Epoch 120/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4724 - accuracy: 0.8207\n",
      "Epoch 121/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4709 - accuracy: 0.8217\n",
      "Epoch 122/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4739 - accuracy: 0.8201\n",
      "Epoch 123/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4749 - accuracy: 0.8195\n",
      "Epoch 124/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4729 - accuracy: 0.8219\n",
      "Epoch 125/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4745 - accuracy: 0.8220\n",
      "Epoch 126/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4639 - accuracy: 0.8239\n",
      "Epoch 127/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4710 - accuracy: 0.8216\n",
      "Epoch 128/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4715 - accuracy: 0.8205\n",
      "Epoch 129/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4659 - accuracy: 0.8234\n",
      "Epoch 130/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4677 - accuracy: 0.8230\n",
      "Epoch 131/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4683 - accuracy: 0.8216\n",
      "Epoch 132/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4669 - accuracy: 0.8236\n",
      "Epoch 133/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4658 - accuracy: 0.8242\n",
      "Epoch 134/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4666 - accuracy: 0.8238\n",
      "Epoch 135/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4713 - accuracy: 0.8209\n",
      "Epoch 136/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4607 - accuracy: 0.8253\n",
      "Epoch 137/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4719 - accuracy: 0.8217\n",
      "Epoch 138/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4650 - accuracy: 0.8261\n",
      "Epoch 139/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4657 - accuracy: 0.8235\n",
      "Epoch 140/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4643 - accuracy: 0.8238\n",
      "Epoch 141/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4689 - accuracy: 0.8235\n",
      "Epoch 142/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4671 - accuracy: 0.8229\n",
      "Epoch 143/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4678 - accuracy: 0.8221\n",
      "Epoch 144/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4652 - accuracy: 0.8251\n",
      "Epoch 145/150\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4655 - accuracy: 0.8226\n",
      "Epoch 146/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4616 - accuracy: 0.8238\n",
      "Epoch 147/150\n",
      "1875/1875 [==============================] - 23s 13ms/step - loss: 0.4655 - accuracy: 0.8234\n",
      "Epoch 148/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4646 - accuracy: 0.8245\n",
      "Epoch 149/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4665 - accuracy: 0.8238\n",
      "Epoch 150/150\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4643 - accuracy: 0.8246\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(9, 9), activation='relu', input_shape=[50, 50, 1]))\n",
    "\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "loss = model.fit(datagen.flow(X_turbo,y_turbo,batch_size=32), epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3060dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')\n",
    "#Train set accuracy 98.52% Test set (simple) accuracy 100.00% Test set (adv.) accuracy 100.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0b9f548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_95 (Conv2D)          (None, 42, 42, 8)         656       \n",
      "                                                                 \n",
      " conv2d_96 (Conv2D)          (None, 40, 40, 8)         584       \n",
      "                                                                 \n",
      " max_pooling2d_45 (MaxPoolin  (None, 20, 20, 8)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_97 (Conv2D)          (None, 18, 18, 16)        1168      \n",
      "                                                                 \n",
      " conv2d_98 (Conv2D)          (None, 16, 16, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d_46 (MaxPoolin  (None, 8, 8, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_99 (Conv2D)          (None, 6, 6, 32)          4640      \n",
      "                                                                 \n",
      " conv2d_100 (Conv2D)         (None, 4, 4, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_101 (Conv2D)         (None, 2, 2, 32)          9248      \n",
      "                                                                 \n",
      " max_pooling2d_47 (MaxPoolin  (None, 1, 1, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,029\n",
      "Trainable params: 28,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/220\n",
      "126/469 [=======>......................] - ETA: 15s - loss: 1.5783 - accuracy: 0.2928"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  MemoryError: Unable to allocate 1.22 MiB for an array with shape (128, 50, 50, 1) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\Kasparas\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\Kasparas\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\Kasparas\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 795, in _get_batches_of_transformed_samples\n    batch_x = np.zeros(\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.22 MiB for an array with shape (128, 50, 50, 1) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[IteratorGetNext/_2]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  MemoryError: Unable to allocate 1.22 MiB for an array with shape (128, 50, 50, 1) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\Kasparas\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\Kasparas\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\Kasparas\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 795, in _get_batches_of_transformed_samples\n    batch_x = np.zeros(\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.22 MiB for an array with shape (128, 50, 50, 1) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_18997246]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [29], line 30\u001b[0m\n\u001b[0;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     26\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr_schedule),\n\u001b[0;32m     27\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m---> 30\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_turbo\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_turbo\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m220\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  MemoryError: Unable to allocate 1.22 MiB for an array with shape (128, 50, 50, 1) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\Kasparas\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\Kasparas\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\Kasparas\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 795, in _get_batches_of_transformed_samples\n    batch_x = np.zeros(\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.22 MiB for an array with shape (128, 50, 50, 1) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[IteratorGetNext/_2]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  MemoryError: Unable to allocate 1.22 MiB for an array with shape (128, 50, 50, 1) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\Kasparas\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\Kasparas\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\Kasparas\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\Kasparas\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 795, in _get_batches_of_transformed_samples\n    batch_x = np.zeros(\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.22 MiB for an array with shape (128, 50, 50, 1) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_18997246]"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(8, kernel_size=(9, 9), activation='relu', input_shape=[50, 50, 1]))\n",
    "\n",
    "model.add(keras.layers.Conv2D(8, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "loss = model.fit(datagen.flow(X_turbo,y_turbo,batch_size=128), epochs=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Featherweight_model.h5')\n",
    "#Train set accuracy 98.51% Test set (simple) accuracy 99.98% Test set (adv.) accuracy 99.98%\n",
    "#2x8 -> 2x16 -> 3x32 150 epochs 16 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b7c1204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_83 (Conv2D)          (None, 42, 42, 8)         656       \n",
      "                                                                 \n",
      " conv2d_84 (Conv2D)          (None, 40, 40, 8)         584       \n",
      "                                                                 \n",
      " max_pooling2d_39 (MaxPoolin  (None, 20, 20, 8)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_85 (Conv2D)          (None, 18, 18, 16)        1168      \n",
      "                                                                 \n",
      " conv2d_86 (Conv2D)          (None, 16, 16, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d_40 (MaxPoolin  (None, 8, 8, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_87 (Conv2D)          (None, 6, 6, 32)          4640      \n",
      "                                                                 \n",
      " conv2d_88 (Conv2D)          (None, 4, 4, 32)          9248      \n",
      "                                                                 \n",
      " max_pooling2d_41 (MaxPoolin  (None, 2, 2, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,261\n",
      "Trainable params: 19,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/220\n",
      "1875/1875 [==============================] - 26s 13ms/step - loss: 1.3698 - accuracy: 0.4060\n",
      "Epoch 2/220\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 1.0993 - accuracy: 0.5526\n",
      "Epoch 3/220\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.9679 - accuracy: 0.6153\n",
      "Epoch 4/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.9038 - accuracy: 0.6442\n",
      "Epoch 5/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.8702 - accuracy: 0.6600\n",
      "Epoch 6/220\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.8390 - accuracy: 0.6746\n",
      "Epoch 7/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.8236 - accuracy: 0.6811\n",
      "Epoch 8/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.7997 - accuracy: 0.6889\n",
      "Epoch 9/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.7812 - accuracy: 0.6981\n",
      "Epoch 10/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.7666 - accuracy: 0.7038\n",
      "Epoch 11/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.7520 - accuracy: 0.7111\n",
      "Epoch 12/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.7415 - accuracy: 0.7148\n",
      "Epoch 13/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.7379 - accuracy: 0.7150\n",
      "Epoch 14/220\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.7256 - accuracy: 0.7206\n",
      "Epoch 15/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.7152 - accuracy: 0.7239\n",
      "Epoch 16/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.7103 - accuracy: 0.7259\n",
      "Epoch 17/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6988 - accuracy: 0.7318\n",
      "Epoch 18/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.6912 - accuracy: 0.7325\n",
      "Epoch 19/220\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.6937 - accuracy: 0.7305\n",
      "Epoch 20/220\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.6861 - accuracy: 0.7351\n",
      "Epoch 21/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.6758 - accuracy: 0.7386\n",
      "Epoch 22/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6760 - accuracy: 0.7377\n",
      "Epoch 23/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6761 - accuracy: 0.7381\n",
      "Epoch 24/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6645 - accuracy: 0.7433\n",
      "Epoch 25/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6608 - accuracy: 0.7452\n",
      "Epoch 26/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6668 - accuracy: 0.7455\n",
      "Epoch 27/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6519 - accuracy: 0.7489\n",
      "Epoch 28/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6624 - accuracy: 0.7439\n",
      "Epoch 29/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6547 - accuracy: 0.7475\n",
      "Epoch 30/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6491 - accuracy: 0.7514\n",
      "Epoch 31/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6525 - accuracy: 0.7480\n",
      "Epoch 32/220\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.6444 - accuracy: 0.7526\n",
      "Epoch 33/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.6398 - accuracy: 0.7541\n",
      "Epoch 34/220\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.6379 - accuracy: 0.7532\n",
      "Epoch 35/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.6445 - accuracy: 0.7516\n",
      "Epoch 36/220\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.6349 - accuracy: 0.7537\n",
      "Epoch 37/220\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.6331 - accuracy: 0.7564\n",
      "Epoch 38/220\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.6301 - accuracy: 0.7561\n",
      "Epoch 39/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.6319 - accuracy: 0.7560\n",
      "Epoch 40/220\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.6247 - accuracy: 0.7602\n",
      "Epoch 41/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.6255 - accuracy: 0.7601\n",
      "Epoch 42/220\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.6253 - accuracy: 0.7576\n",
      "Epoch 43/220\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.6243 - accuracy: 0.7590\n",
      "Epoch 44/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.6202 - accuracy: 0.7613\n",
      "Epoch 45/220\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.6232 - accuracy: 0.7611\n",
      "Epoch 46/220\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.6182 - accuracy: 0.7609\n",
      "Epoch 47/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6199 - accuracy: 0.7594\n",
      "Epoch 48/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6116 - accuracy: 0.7648\n",
      "Epoch 49/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6106 - accuracy: 0.7646\n",
      "Epoch 50/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6123 - accuracy: 0.7635\n",
      "Epoch 51/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6148 - accuracy: 0.7624\n",
      "Epoch 52/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6132 - accuracy: 0.7640\n",
      "Epoch 53/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6077 - accuracy: 0.7681\n",
      "Epoch 54/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.6082 - accuracy: 0.7660\n",
      "Epoch 55/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6075 - accuracy: 0.7675\n",
      "Epoch 56/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6051 - accuracy: 0.7668\n",
      "Epoch 57/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6004 - accuracy: 0.7681\n",
      "Epoch 58/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6082 - accuracy: 0.7653\n",
      "Epoch 59/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.6027 - accuracy: 0.7683\n",
      "Epoch 60/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6019 - accuracy: 0.7673\n",
      "Epoch 61/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.6022 - accuracy: 0.7686\n",
      "Epoch 62/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5978 - accuracy: 0.7692\n",
      "Epoch 63/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5998 - accuracy: 0.7686\n",
      "Epoch 64/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5980 - accuracy: 0.7703\n",
      "Epoch 65/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5959 - accuracy: 0.7714\n",
      "Epoch 66/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5988 - accuracy: 0.7696\n",
      "Epoch 67/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5890 - accuracy: 0.7739\n",
      "Epoch 68/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5965 - accuracy: 0.7717\n",
      "Epoch 69/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5957 - accuracy: 0.7709\n",
      "Epoch 70/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5918 - accuracy: 0.7704\n",
      "Epoch 71/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5944 - accuracy: 0.7689\n",
      "Epoch 72/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5876 - accuracy: 0.7759\n",
      "Epoch 73/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5909 - accuracy: 0.7737\n",
      "Epoch 74/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5837 - accuracy: 0.7756\n",
      "Epoch 75/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5890 - accuracy: 0.7736\n",
      "Epoch 76/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5913 - accuracy: 0.7736\n",
      "Epoch 77/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5863 - accuracy: 0.7748\n",
      "Epoch 78/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5916 - accuracy: 0.7737\n",
      "Epoch 79/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5846 - accuracy: 0.7744\n",
      "Epoch 80/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5846 - accuracy: 0.7753\n",
      "Epoch 81/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5778 - accuracy: 0.7788\n",
      "Epoch 82/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5797 - accuracy: 0.7794\n",
      "Epoch 83/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5791 - accuracy: 0.7776\n",
      "Epoch 84/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5846 - accuracy: 0.7755\n",
      "Epoch 85/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5868 - accuracy: 0.7756\n",
      "Epoch 86/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5826 - accuracy: 0.7752\n",
      "Epoch 87/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5801 - accuracy: 0.7766\n",
      "Epoch 88/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5841 - accuracy: 0.7756\n",
      "Epoch 89/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5778 - accuracy: 0.7802\n",
      "Epoch 90/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5700 - accuracy: 0.7800\n",
      "Epoch 91/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5691 - accuracy: 0.7822\n",
      "Epoch 92/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5762 - accuracy: 0.7798\n",
      "Epoch 93/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5752 - accuracy: 0.7804\n",
      "Epoch 94/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5744 - accuracy: 0.7775\n",
      "Epoch 95/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5763 - accuracy: 0.7773\n",
      "Epoch 96/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5759 - accuracy: 0.7777\n",
      "Epoch 97/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5777 - accuracy: 0.7782\n",
      "Epoch 98/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5717 - accuracy: 0.7796\n",
      "Epoch 99/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5717 - accuracy: 0.7796\n",
      "Epoch 100/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5754 - accuracy: 0.7783\n",
      "Epoch 101/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5713 - accuracy: 0.7810\n",
      "Epoch 102/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5725 - accuracy: 0.7788\n",
      "Epoch 103/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5649 - accuracy: 0.7850\n",
      "Epoch 104/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5673 - accuracy: 0.7812\n",
      "Epoch 105/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5705 - accuracy: 0.7780\n",
      "Epoch 106/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5732 - accuracy: 0.7793\n",
      "Epoch 107/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5707 - accuracy: 0.7809\n",
      "Epoch 108/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5778 - accuracy: 0.7789\n",
      "Epoch 109/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5728 - accuracy: 0.7796\n",
      "Epoch 110/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5672 - accuracy: 0.7829\n",
      "Epoch 111/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5665 - accuracy: 0.7803\n",
      "Epoch 112/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5763 - accuracy: 0.7772\n",
      "Epoch 113/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5692 - accuracy: 0.7814\n",
      "Epoch 114/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5654 - accuracy: 0.7824\n",
      "Epoch 115/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5603 - accuracy: 0.7842\n",
      "Epoch 116/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5658 - accuracy: 0.7817\n",
      "Epoch 117/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5696 - accuracy: 0.7800\n",
      "Epoch 118/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5710 - accuracy: 0.7802\n",
      "Epoch 119/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5613 - accuracy: 0.7849\n",
      "Epoch 120/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5637 - accuracy: 0.7824\n",
      "Epoch 121/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5628 - accuracy: 0.7829\n",
      "Epoch 122/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5672 - accuracy: 0.7806\n",
      "Epoch 123/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5637 - accuracy: 0.7824\n",
      "Epoch 124/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5656 - accuracy: 0.7824\n",
      "Epoch 125/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5619 - accuracy: 0.7824\n",
      "Epoch 126/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5600 - accuracy: 0.7843\n",
      "Epoch 127/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5603 - accuracy: 0.7849\n",
      "Epoch 128/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5558 - accuracy: 0.7890\n",
      "Epoch 129/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5575 - accuracy: 0.7857\n",
      "Epoch 130/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5642 - accuracy: 0.7833\n",
      "Epoch 131/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5654 - accuracy: 0.7824\n",
      "Epoch 132/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5642 - accuracy: 0.7839\n",
      "Epoch 133/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5609 - accuracy: 0.7836\n",
      "Epoch 134/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5554 - accuracy: 0.7858\n",
      "Epoch 135/220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5580 - accuracy: 0.7863\n",
      "Epoch 136/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5552 - accuracy: 0.7854\n",
      "Epoch 137/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5621 - accuracy: 0.7836\n",
      "Epoch 138/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5605 - accuracy: 0.7833\n",
      "Epoch 139/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5547 - accuracy: 0.7857\n",
      "Epoch 140/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5558 - accuracy: 0.7849\n",
      "Epoch 141/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5561 - accuracy: 0.7879\n",
      "Epoch 142/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5564 - accuracy: 0.7855\n",
      "Epoch 143/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5577 - accuracy: 0.7862\n",
      "Epoch 144/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5574 - accuracy: 0.7854\n",
      "Epoch 145/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5528 - accuracy: 0.7880\n",
      "Epoch 146/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5516 - accuracy: 0.7889\n",
      "Epoch 147/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5568 - accuracy: 0.7858\n",
      "Epoch 148/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5561 - accuracy: 0.7868\n",
      "Epoch 149/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5564 - accuracy: 0.7871\n",
      "Epoch 150/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5560 - accuracy: 0.7876\n",
      "Epoch 151/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5540 - accuracy: 0.7870\n",
      "Epoch 152/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5554 - accuracy: 0.7846\n",
      "Epoch 153/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5563 - accuracy: 0.7850\n",
      "Epoch 154/220\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.5528 - accuracy: 0.7886\n",
      "Epoch 155/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5514 - accuracy: 0.7880\n",
      "Epoch 156/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5464 - accuracy: 0.7896\n",
      "Epoch 157/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5540 - accuracy: 0.7871\n",
      "Epoch 158/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5496 - accuracy: 0.7867\n",
      "Epoch 159/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5494 - accuracy: 0.7904\n",
      "Epoch 160/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5517 - accuracy: 0.7896\n",
      "Epoch 161/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5540 - accuracy: 0.7879\n",
      "Epoch 162/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5485 - accuracy: 0.7894\n",
      "Epoch 163/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5479 - accuracy: 0.7878\n",
      "Epoch 164/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5482 - accuracy: 0.7912\n",
      "Epoch 165/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5509 - accuracy: 0.7875\n",
      "Epoch 166/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5480 - accuracy: 0.7867\n",
      "Epoch 167/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5488 - accuracy: 0.7888\n",
      "Epoch 168/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5464 - accuracy: 0.7904\n",
      "Epoch 169/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5462 - accuracy: 0.7903\n",
      "Epoch 170/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5487 - accuracy: 0.7898\n",
      "Epoch 171/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5423 - accuracy: 0.7903\n",
      "Epoch 172/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5434 - accuracy: 0.7906\n",
      "Epoch 173/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5458 - accuracy: 0.7886\n",
      "Epoch 174/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5491 - accuracy: 0.7887\n",
      "Epoch 175/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5465 - accuracy: 0.7900\n",
      "Epoch 176/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5422 - accuracy: 0.7918\n",
      "Epoch 177/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5505 - accuracy: 0.7889\n",
      "Epoch 178/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5473 - accuracy: 0.7901\n",
      "Epoch 179/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5465 - accuracy: 0.7909\n",
      "Epoch 180/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5460 - accuracy: 0.7888\n",
      "Epoch 181/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5435 - accuracy: 0.7896\n",
      "Epoch 182/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5430 - accuracy: 0.7914\n",
      "Epoch 183/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5422 - accuracy: 0.7884\n",
      "Epoch 184/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5447 - accuracy: 0.7926\n",
      "Epoch 185/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5445 - accuracy: 0.7918\n",
      "Epoch 186/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5394 - accuracy: 0.7926\n",
      "Epoch 187/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5388 - accuracy: 0.7932\n",
      "Epoch 188/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5430 - accuracy: 0.7903\n",
      "Epoch 189/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5393 - accuracy: 0.7913\n",
      "Epoch 190/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5410 - accuracy: 0.7912\n",
      "Epoch 191/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5418 - accuracy: 0.7919\n",
      "Epoch 192/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5407 - accuracy: 0.7926\n",
      "Epoch 193/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5417 - accuracy: 0.7905\n",
      "Epoch 194/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5422 - accuracy: 0.7914\n",
      "Epoch 195/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5440 - accuracy: 0.7910\n",
      "Epoch 196/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5480 - accuracy: 0.7906\n",
      "Epoch 197/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5416 - accuracy: 0.7919\n",
      "Epoch 198/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5443 - accuracy: 0.7906\n",
      "Epoch 199/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5374 - accuracy: 0.7945\n",
      "Epoch 200/220\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5391 - accuracy: 0.7933\n",
      "Epoch 201/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5406 - accuracy: 0.7924\n",
      "Epoch 202/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5429 - accuracy: 0.7905\n",
      "Epoch 203/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5435 - accuracy: 0.7897\n",
      "Epoch 204/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5418 - accuracy: 0.7907\n",
      "Epoch 205/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5406 - accuracy: 0.7920\n",
      "Epoch 206/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5375 - accuracy: 0.7928\n",
      "Epoch 207/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5407 - accuracy: 0.7922\n",
      "Epoch 208/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5416 - accuracy: 0.7923\n",
      "Epoch 209/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5330 - accuracy: 0.7953\n",
      "Epoch 210/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5349 - accuracy: 0.7955\n",
      "Epoch 211/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5368 - accuracy: 0.7936\n",
      "Epoch 212/220\n",
      "1875/1875 [==============================] - 23s 13ms/step - loss: 0.5426 - accuracy: 0.7897\n",
      "Epoch 213/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5389 - accuracy: 0.7933\n",
      "Epoch 214/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5431 - accuracy: 0.7919\n",
      "Epoch 215/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5411 - accuracy: 0.7947\n",
      "Epoch 216/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5348 - accuracy: 0.7947\n",
      "Epoch 217/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5370 - accuracy: 0.7927\n",
      "Epoch 218/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5357 - accuracy: 0.7939\n",
      "Epoch 219/220\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.5347 - accuracy: 0.7941\n",
      "Epoch 220/220\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5382 - accuracy: 0.7907\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(8, kernel_size=(9, 9), activation='relu', input_shape=[50, 50, 1]))\n",
    "\n",
    "model.add(keras.layers.Conv2D(8, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "loss = model.fit(datagen.flow(X_turbo,y_turbo,batch_size=32), epochs=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52116c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Featherweight_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f79537f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_89 (Conv2D)          (None, 40, 40, 8)         976       \n",
      "                                                                 \n",
      " conv2d_90 (Conv2D)          (None, 38, 38, 8)         584       \n",
      "                                                                 \n",
      " max_pooling2d_42 (MaxPoolin  (None, 19, 19, 8)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_91 (Conv2D)          (None, 17, 17, 16)        1168      \n",
      "                                                                 \n",
      " conv2d_92 (Conv2D)          (None, 15, 15, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d_43 (MaxPoolin  (None, 7, 7, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_93 (Conv2D)          (None, 5, 5, 32)          4640      \n",
      "                                                                 \n",
      " conv2d_94 (Conv2D)          (None, 3, 3, 32)          9248      \n",
      "                                                                 \n",
      " max_pooling2d_44 (MaxPoolin  (None, 1, 1, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,101\n",
      "Trainable params: 19,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 572. MiB for an array with shape (60000, 50, 50, 1) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 28\u001b[0m\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr_schedule),\n\u001b[0;32m     25\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m---> 28\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_turbo\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_turbo\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m220\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py:1547\u001b[0m, in \u001b[0;36mImageDataGenerator.flow\u001b[1;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, ignore_class_split, subset)\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow\u001b[39m(\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1490\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1501\u001b[0m ):\n\u001b[0;32m   1502\u001b[0m     \u001b[38;5;124;03m\"\"\"Takes data & label arrays, generates batches of augmented data.\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \n\u001b[0;32m   1504\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1545\u001b[0m \n\u001b[0;32m   1546\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNumpyArrayIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_class_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_class_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\image.py:755\u001b[0m, in \u001b[0;36mNumpyArrayIterator.__init__\u001b[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, ignore_class_split, dtype)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m             y \u001b[38;5;241m=\u001b[39m y[split_idx:]\n\u001b[1;32m--> 755\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_misc \u001b[38;5;241m=\u001b[39m x_misc\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 572. MiB for an array with shape (60000, 50, 50, 1) and data type float32"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(8, kernel_size=(11, 11), activation='relu', input_shape=[50, 50, 1]))\n",
    "\n",
    "model.add(keras.layers.Conv2D(8, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "loss = model.fit(datagen.flow(X_turbo,y_turbo,batch_size=32), epochs=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab36ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Featherweight_model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc71e805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
