{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92457403",
   "metadata": {},
   "source": [
    "Since I have 3 graveyards of tested models in my AI primer repository and the process of working towards the 100% 100% was rather hectic and all over the place, I am writing a new file to showcase progress from recollection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582859db",
   "metadata": {},
   "source": [
    "I have found a post by Andrej Karpathy that would have helped me http://karpathy.github.io/2019/04/25/recipe/. Though this is just one of many things I've found during the journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f1f91",
   "metadata": {},
   "source": [
    "## Get that GPU working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187896a1",
   "metadata": {},
   "source": [
    "First challenge was to install TensorFlow locally in a way so it would recognise the GPU in my computer and so the training would become as fast as possible. Took me longer than I'd like to admit. Long story short, https://www.tensorflow.org/install/pip is the way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fdf58",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209431c",
   "metadata": {},
   "source": [
    "I looked at the data. Standardized it, checked the shapes of the arrays I was going to be working with, plotted some of the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4377605",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('flatland_train.npz')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "y[y != 0] -= 2\n",
    "X = X / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5abc49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    ax = plt.subplot(5,6,i+1)\n",
    "    \n",
    "    ax.axis('off')\n",
    "    plt.imshow(X[i],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13961d7",
   "metadata": {},
   "source": [
    "Worthy to notice, I should learn to spend more time on getting to know the data. According to Karpathy at least. On the other hand, this dataset was clear and clean enough so that I could skip this crucial step. You could say the dataset, as in many instances in learning, was spoonfed to me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a4e4b",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f0648",
   "metadata": {},
   "source": [
    "I knew right from the start that I was going to have to augment the data to arrive at a perfect result, but I wanted to first see how far I could get without touching the data and only building the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c423880",
   "metadata": {},
   "source": [
    "I came into the project not even knowing what CNNs were, so naturally, I started off with just regular deep layers. The architecture:\n",
    "\n",
    "Dense 2000 relu\n",
    "\n",
    "Dropout 0.2\n",
    "\n",
    "Dense 1000 relu\n",
    "\n",
    "Dropout 0.2\n",
    "\n",
    "Dense 500 relu\n",
    "\n",
    "Dropout 0.2\n",
    "\n",
    "Dense 250 relu\n",
    "\n",
    "Dropout 0.2\n",
    "\n",
    "Dense out softmax\n",
    "\n",
    "That's fat. 7,630,005 weights. Bad results. I recognised I had to dive into CNNs. Before CNNs I wanted to better understand neural networks in general. So I did. [3b1b series on neural networks](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) was a great starting point for a deeper understanding. \n",
    "\n",
    "Some CNN reading: https://deeplizard.com/learn/video/YRhxdVk_sIs, http://colah.github.io/, https://developers.google.com/machine-learning/guides/rules-of-ml\n",
    "\n",
    "More than working on the project in the first stages, I did reading on convolutional layers and pooling. Then I came back to building my own little thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd709aa",
   "metadata": {},
   "source": [
    "Still ignorant to the details of building an architecture, I brute-forced a starting model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=[50, 50, 1]))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "#loss = model.fit(X, y, epochs=8, batch_size=16, validation_split=0.2)\n",
    "\n",
    "#pd.DataFrame(loss.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc987d",
   "metadata": {},
   "source": [
    "Train set accuracy 98.33% Test set (simple) accuracy 99.54% Test set (adv.) accuracy 85.39%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09df86",
   "metadata": {},
   "source": [
    "Without any data augmentation we arrived at a decent result. Still though. 2,435,370 weights. Not the most lightweight model. Main takeaway from this was that convolutional layers work and how they work. Pooling was a new discovery for me as well. Good insight.\n",
    "\n",
    "Looking from the current perspective into that recent model - dropout after convolutional layers seems wrong. Many convolutional layers without pooling inbetween seems wrong. Extremely small batch size seems sub-optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b015c",
   "metadata": {},
   "source": [
    "## Augment that data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b58ccb",
   "metadata": {},
   "source": [
    "First reference article:\n",
    "https://arxiv.org/pdf/1003.0358.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b7adb",
   "metadata": {},
   "source": [
    "I was going to work with data augmented in batches. So I could make a bigger dataset. Fix the shape of the dataset, so that the model would eat it up nicely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_turbo=np.concatenate((X,X,X,X,X,X))\n",
    "y_turbo=np.concatenate((y,y,y,y,y,y))\n",
    "X_turbo = np.expand_dims(X_turbo, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81977a6e",
   "metadata": {},
   "source": [
    "Long story short: [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator). Don't forget to apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.55,\n",
    "    height_shift_range=0.55,\n",
    "    zoom_range = [0.35, 1.65],\n",
    "    shear_range = 13,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68bb40",
   "metadata": {},
   "source": [
    "I just learned to augment my pictures. Neat. The results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc09cae",
   "metadata": {},
   "source": [
    "(Don't forget to apply the augmentation when building the model by using datagen.flow as seen in the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=[50, 50, 1]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "#loss = model.fit(datagen.flow(X_aug,y,batch_size=8), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd67f4d9",
   "metadata": {},
   "source": [
    "Train set accuracy 98.52% Test set (simple) accuracy 100.00% Test set (adv.) accuracy 98.30%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9357743d",
   "metadata": {},
   "source": [
    "Woah! That was easy. I thought I was champion. Well turns out I was not. 272,682 weights. Pretty light. I liked this. \n",
    "\n",
    "I look at it now and recognise that the amount of naked convolutional layers is disgusting and the model is faaar from perfect. But this was the moment where I started to work for the last few percent to arrive at 100% 100%. And so the main part of the work and reading came after this moment. I would say that working towards 100 100 after this moment was the most insightful and useful as a student. I now started turning my eye to different neural network architectures. By far the most interesting topic for me in the project. And so began my journey of self-discoveries again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f344907",
   "metadata": {},
   "source": [
    "From here I was reading - improving - reading - improving and so on and so forth until I had a reasonable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143655f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=[50, 50, 1]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(keras.layers.Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "#loss = model.fit(datagen.flow(X_turbo,y_turbo,batch_size=4), epochs=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f410c27",
   "metadata": {},
   "source": [
    "Boy is getting the right idea. Pooling after each few convolutional layers is a standard procedure in most (if not all(?)) architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665794d4",
   "metadata": {},
   "source": [
    "The recent model did not work out. Or at least gave worse results when posting on the link. Made me go back to my old ways of brute-forcing it: \"Wait. This doesn't work. I want to go back to the old model.\" So I did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78782917",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=[50, 50, 1]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "#loss = model.fit(datagen.flow(X_turbo,y_turbo,batch_size=128), epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4936a",
   "metadata": {},
   "source": [
    "Train set accuracy 98.52% Test set (simple) accuracy 100.00% Test set (adv.) accuracy 99.95%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00718e52",
   "metadata": {},
   "source": [
    "Damn. This must really be a good model. It wasn't a good model. Well it wasn't awful because it worked. On the other hand, I am now glad that this model did not give me 100% 100% immediately. Once again, not getting 100 100 pushed me to learn and I am now extremely glad it did. If I had gotten the 100 100 with this weird model, I would have stopped at this point.\n",
    "\n",
    "After this I went on to decrease my result for a better architecture in return. I started trying to copy other architectures and looking for inspiration online. I started playing around with different architectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d10049",
   "metadata": {},
   "source": [
    "Some reading: https://theaisummer.com/cnn-architectures/, https://paperswithcode.com/sota/image-classification-on-imagenet, https://github.com/pranshushah/cifar-10_resnet_tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762c980",
   "metadata": {},
   "source": [
    "Worthy to note, I was inspired to copy resnet. But I couldn't get residual connections to work in keras and there is not a predefined residual connection layer or option in keras so I had to go back to the simple ways for the time being. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146f716",
   "metadata": {},
   "source": [
    "## The evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b083194e",
   "metadata": {},
   "source": [
    "Since I have changed the models in the graveyards so much, I don't have an exact evolution of them from the most recent one shown above to the final one. I will try to write what changes to the architecture worked and the thought process behind them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ca8aa",
   "metadata": {},
   "source": [
    "### Remove dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711dfaad",
   "metadata": {},
   "source": [
    "Dense layers add in too much weight to the model and do little of the job. I found out pretty quickly by experimenting that dense layers are absolutely redundant for this task. In fact, in this case, they actually make the model worse. Removed them completely and started working with convolutional layers only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a29830",
   "metadata": {},
   "source": [
    "### Double filter number for conv layers after each pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae788f",
   "metadata": {},
   "source": [
    "Following the usual shape of many top architectures, I realised the amount of filters should go double up after each pooling. Or at least the amount of filters should always go up, not down. The final model, as shown below, goes 2x16 -> 2x32 -> 2x64 between pooling. I usually enjoyed 2x32 -> 2x64 -> 1x128 as a nice balance between amount of weights and power of model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b5ee4",
   "metadata": {},
   "source": [
    "### The majority of work is done in the first layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faca6fa",
   "metadata": {},
   "source": [
    "We are classifying polygons. If we think about what we learned about convolutional networks, we may realise that the most basic shapes in images are found out by the first layers of the network. Therefore such logic and the simplicity of the objects we are classifying made me realise I should put emphasis on the first layers of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fac8bf8",
   "metadata": {},
   "source": [
    "### Size up those first layer filters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0367c1e9",
   "metadata": {},
   "source": [
    "The importance of first layers made me start playing with the size of their filters. I quickly found that increasing the size of the first layer filters improves the model manyfold. Even one single layer neural network with sufficiently big filter size goes up in accuracy quite fast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(13, 13), activation='relu', input_shape=[50, 50, 1]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8a97a",
   "metadata": {},
   "source": [
    "Something like this usually does surprisingly well for the task. What was left now was to use this as a foundation to build something proper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887cc717",
   "metadata": {},
   "source": [
    "First - the beefy filter layer likes a partner layer. The partner could have a big filter size as well (like 9x9 in the case above). But later I settled for just the regular size for that second filter - (3x3). MaxPooling. 2 regular conv layers. Maxpooling. 1 or 2 regular conv layers. Maxpooling. Flatten. Output dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6930772e",
   "metadata": {},
   "source": [
    "This was more or less the architecture I found to be the best. Although I would experiment by adding a dense layer, playing with various layers, normalisation, dropout, changing batch size, epoch number, WORKING WITH LEARNING RATE, I would always come back to the simple architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ccb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(13, 13), activation='relu', input_shape=[50, 50, 1]))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd6356",
   "metadata": {},
   "source": [
    "### GlobalPooling trained models don't work with the link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d4e5d",
   "metadata": {},
   "source": [
    "For some reason if the model is trained using GlobalPooling after all the conv layers, the [link](https://us-central1-aiprimer.cloudfunctions.net/flatland?model_link=) breaks. I don't know if I did something wrong by trying globalpooling or what the case was. Model trains fine. Link doesn't seem to take such a model in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543cfb75",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b0f16",
   "metadata": {},
   "source": [
    "Although 3e-4 for the Adam optimizer does really well, I have played my own game with the learning rate. Through various tries I have come to the conclusion that a learning rate defined by an exponential decay scheduler works wonders for my case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6155498",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 4e-4\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.98,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6706db",
   "metadata": {},
   "source": [
    "Though I know now that learning rate is probably the most important hyperparameter in a model. There is always work to be done to tweak the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ad287",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61186e33",
   "metadata": {},
   "source": [
    "I got really close many times. I knew it was more or less a matter of luck in tweaking the numbers more or less to get the wanted hundreds. What came was many different attempts with small changes. The following model was the one that got the perfect result and is as lightweight as I could find it to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(9, 9), activation='relu', input_shape=[50, 50, 1]))\n",
    "\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa48aa",
   "metadata": {},
   "source": [
    "Train set accuracy 98.52% Test set (simple) accuracy 100.00% Test set (adv.) accuracy 100.00%\n",
    "\n",
    "952 KB. 75,514 weights. \n",
    "\n",
    "Lovely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2f8b1",
   "metadata": {},
   "source": [
    "As of now I am, still building a smaller model yet. I call it Featherweight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929a7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(8, kernel_size=(9, 9), activation='relu', input_shape=[50, 50, 1]))\n",
    "\n",
    "model.add(keras.layers.Conv2D(8, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea83b1",
   "metadata": {},
   "source": [
    "Train set accuracy 98.50% Test set (simple) accuracy 99.97% Test set (adv.) accuracy 99.97%\n",
    "\n",
    "301KB. 19,906 weights.\n",
    "\n",
    "Sweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b387dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
